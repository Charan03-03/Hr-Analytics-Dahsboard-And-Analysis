# -*- coding: utf-8 -*-
"""Hr Analytics Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16qtI4J3V4SAfJBsler-hVuDZFO52vJwH
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tabulate import tabulate

# Set visualization style
sns.set_style("whitegrid")

df=pd.read_csv('HR_Analytics.csv')
df

# --- 2. Data Cleanup and Imputation ---
# Dropping constant/identifier/derived columns
cols_to_drop = ['EmployeeCount', 'StandardHours', 'EmployeeNumber', 'Over18', 'EmpID', 'AgeGroup', 'SalarySlab']
df = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')
df

# Handle Missing Values in 'YearsWithCurrManager' by imputation (median)
median_years = df['YearsWithCurrManager'].median()
df['YearsWithCurrManager'] = df['YearsWithCurrManager'].fillna(median_years)

# Convert Attrition to numeric (1/0) for analysis
df['Attrition_Numeric'] = df['Attrition'].map({'Yes': 1, 'No': 0})

# --- 3. Target Variable Analysis (Attrition Rate) ---
plt.figure(figsize=(6, 4))
sns.countplot(x='Attrition', data=df, palette='viridis')
plt.title('Distribution of Employee Attrition (Yes/No)')
plt.show()

attrition_rate = df['Attrition'].value_counts(normalize=True) * 100
print(f"Attrition Rate (Yes): {attrition_rate['Yes']:.2f}%")

# --- 4. Bivariate Analysis (Causal Categorical Drivers) ---
categorical_drivers = ['Department', 'JobRole', 'JobLevel', 'EducationField', 'OverTime', 'BusinessTravel']

plt.figure(figsize=(18, 15))
plt.suptitle('Attrition Rate by Key Categorical Drivers', fontsize=16)

for i, col in enumerate(categorical_drivers):
    plt.subplot(3, 2, i + 1)
    # Calculate the percentage of attrition (mean of Attrition_Numeric)
    attrition_pct = df.groupby(col)['Attrition_Numeric'].mean() * 100

    sns.barplot(x=attrition_pct.index, y=attrition_pct.values, palette='magma')
    plt.title(f'Attrition Rate (%) by {col}', fontsize=12)
    plt.ylabel('Attrition Rate (%)')
    plt.xlabel(col)
    plt.xticks(rotation=45, ha='right')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# --- 5. Bivariate Analysis (Numeric/Satisfaction Drivers) ---
numeric_drivers = ['MonthlyIncome', 'YearsAtCompany', 'Age', 'JobSatisfaction', 'EnvironmentSatisfaction']

plt.figure(figsize=(18, 10))
plt.suptitle('Attrition Analysis on Key Numeric/Satisfaction Factors', fontsize=16)

for i, col in enumerate(numeric_drivers):
    plt.subplot(2, 3, i + 1)
    if col in ['JobSatisfaction', 'EnvironmentSatisfaction']:
        # For satisfaction scores, plot the rate
        attrition_rate = df.groupby(col)['Attrition_Numeric'].mean() * 100
        sns.barplot(x=attrition_rate.index, y=attrition_rate.values, palette='Blues')
        plt.title(f'Attrition Rate (%) by {col}')
    else:
        # For continuous variables (Income, Tenure, Age), use a box plot
        sns.boxplot(x='Attrition', y=col, data=df, palette='Purples')
        plt.title(f'{col} Distribution by Attrition')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# --- 6. Correlation Analysis (Printing only the top findings) ---
numeric_corr_df = df.select_dtypes(include=[np.number])
attrition_corr = numeric_corr_df.corr()['Attrition_Numeric'].sort_values(ascending=False)

print("\n--- ðŸ“Š Top 10 Retention and Attrition Correlates ---")
print("Top 5 Retention Factors (Negative Correlation - Pushes towards Staying):")
print(tabulate(attrition_corr[-5:].reset_index(), headers=['Feature', 'Correlation'], tablefmt='plain', floatfmt=".3f"))

print("\nTop 5 Attrition Factors (Positive Correlation - Pushes towards Leaving):")
print(tabulate(attrition_corr[1:6].reset_index(), headers=['Feature', 'Correlation'], tablefmt='plain', floatfmt=".3f"))

# Save the final cleaned data for the next step (Model Training)
df.to_csv('hr_analytics_cleaned_final.csv', index=False)

"""**Logistic Regression Code: Training & Evaluation**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# --- 1. Load Data and Preparation ---
# Load the cleaned data from the EDA step
df = pd.read_csv('hr_analytics_cleaned_final.csv')

# Separate features (X) and target (y)
X = df.drop(['Attrition', 'Attrition_Numeric'], axis=1)
y = df['Attrition_Numeric']

# Identify column types
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = X.select_dtypes(include=['object']).columns

# Create Preprocessing Pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols), # Scaling numerical features
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols) # Encoding categorical features
    ],
    remainder='drop'
)

# Apply Preprocessing and Convert back to DataFrame for feature names
X_processed = preprocessor.fit_transform(X)
cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)
feature_names = list(numerical_cols) + list(cat_feature_names)
X_final = pd.DataFrame(X_processed, columns=feature_names)

# --- 2. Data Split ---
# Split into Training and Testing sets (using stratify due to the 16.08% imbalance)
X_train, X_test, y_train, y_test = train_test_split(
    X_final, y, test_size=0.3, random_state=42, stratify=y
)

# --- 3. Model Training ---
# Using class_weight='balanced' helps the model learn from the minority class (Attrition=Yes)
log_model = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced')
log_model.fit(X_train, y_train)

# --- 4. Model Evaluation and Deliverables ---
y_pred = log_model.predict(X_test)

# Confusion Matrix (DELIVERABLE: PNG file)
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds',
            xticklabels=['Stay (0)', 'Leave (1)'],
            yticklabels=['Actual Stay (0)', 'Actual Leave (1)'])
plt.title('Confusion Matrix: Logistic Regression')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.savefig('confusion_matrix.png')
plt.close()

# Classification Report (DELIVERABLE: CSV file)
report = classification_report(y_test, y_pred, output_dict=True)
report_df = pd.DataFrame(report).transpose()
report_df.to_csv('model_accuracy_report.csv', index=True)

# Save the necessary components for the next SHAP analysis step
X_test.to_csv('X_test_for_shap.csv', index=False)
with open('log_model.pkl', 'wb') as file:
    pickle.dump(log_model, file)

import pandas as pd
import shap
import pickle
import matplotlib.pyplot as plt
import numpy as np

# --- 1. Load the saved model and test data ---
try:
    with open('log_model.pkl', 'rb') as file:
        log_model = pickle.load(file)
except FileNotFoundError:
    print("Error: 'log_model.pkl' not found. Please ensure the Logistic Regression code ran successfully.")
    exit()

X_test = pd.read_csv('X_test_for_shap.csv')

# --- 2. Define Samples and Convert to NumPy Arrays ---
# Sample data for explanation (the 100 rows we want to explain)
N_EXPLAIN = 100
X_explain_df = X_test.sample(n=N_EXPLAIN, random_state=42).reset_index(drop=True)

# Convert to NumPy array for robustness
X_explain_np = X_explain_df.values

# Define the background data (using shap.sample)
X_background_df = shap.sample(X_test, 50)
X_background_np = X_background_df.values


# --- 3. SHAP Calculation ---
# 3a. Initialize Explainer: Use NumPy array for background
explainer = shap.KernelExplainer(log_model.predict_proba, X_background_np)

# 3b. Calculate SHAP values for class 1 (Attrition=Yes)
# Corrected line to get SHAP values for all samples, all features, for class 1
shap_values = explainer.shap_values(X_explain_np)[:, :, 1]

# --- Diagnostic Check ---
print(f"Shape of X_explain_np: {X_explain_np.shape}")
print(f"Shape of shap_values: {shap_values.shape}")


# --- 4. Global Feature Importance (SHAP Summary Plot) ---
plt.figure(figsize=(10, 8))
# Pass the original DataFrame (X_explain_df) to 'features' for correct axis labels,
# and the calculated values to shap_values
shap.summary_plot(shap_values, features=X_explain_df, show=False)
plt.title('SHAP Summary Plot: Global Attrition Drivers')
plt.tight_layout()
plt.savefig('shap_summary_plot.png')
plt.close()


# --- 5. Save Top SHAP Factors (for PDF Report) ---
# Calculate mean absolute SHAP value for each feature
shap_df = pd.DataFrame(
    {'Feature': X_explain_df.columns,
     'SHAP_Impact': np.abs(shap_values).mean(axis=0)}
).sort_values(by='SHAP_Impact', ascending=False)

# Save the top 10 factors
top_shap_factors = shap_df.head(10)
top_shap_factors.to_csv('top_shap_factors.csv', index=False)

print("\nSHAP analysis complete. Key files saved:")
print("- shap_summary_plot.png")
print("- top_shap_factors.csv")

